#!/bin/bash

# ðŸŽ¯ Quick Test Classification Guide - ZentraQMS
# Script de ayuda rÃ¡pida para clasificar tests

echo "ðŸŽ¯ ZentraQMS - GuÃ­a RÃ¡pida de ClasificaciÃ³n de Tests"
echo "=================================================="
echo ""

# FunciÃ³n para mostrar ayuda
show_help() {
    echo "ðŸ“‹ COMANDOS DISPONIBLES:"
    echo ""
    echo "ðŸ” ANÃLISIS:"
    echo "  ./scripts/quick-test-guide.sh analyze     - Analizar todos los tests"
    echo "  ./scripts/quick-test-guide.sh check       - Verificar tests crÃ­ticos"
    echo "  ./scripts/quick-test-guide.sh status      - Estado actual del proyecto"
    echo ""
    echo "ðŸ“ DESARROLLO:"
    echo "  ./scripts/quick-test-guide.sh new-module  - Template para nuevo mÃ³dulo"
    echo "  ./scripts/quick-test-guide.sh classify    - Clasificar test especÃ­fico"
    echo ""
    echo "ðŸ§ª TESTING:"
    echo "  ./scripts/quick-test-guide.sh critical    - Ejecutar solo tests crÃ­ticos"
    echo "  ./scripts/quick-test-guide.sh smart       - Ejecutar con lÃ³gica inteligente"
    echo "  ./scripts/quick-test-guide.sh report      - Generar reporte completo"
}

# FunciÃ³n para analizar tests
analyze_tests() {
    echo "ðŸ” Analizando tests del proyecto..."
    node scripts/classify-tests.js
    echo ""
    echo "ðŸ“ Archivos generados en /tmp/zentraqms-test-config/"
    echo "ðŸ“‹ Revisa classification-report.md para detalles"
}

# FunciÃ³n para verificar tests crÃ­ticos
check_critical() {
    echo "ðŸŽ¯ Verificando tests crÃ­ticos..."
    echo ""
    
    echo "ðŸ”§ Backend:"
    cd backend && python manage.py test --verbosity=1 | head -20
    
    echo ""
    echo "ðŸŽ¨ Frontend crÃ­ticos:"
    cd ../frontend && npm run test:critical 2>/dev/null | tail -10
    
    echo ""
    echo "âœ… Si ambos pasan al 100%, estÃ¡s listo para deployment!"
}

# FunciÃ³n para mostrar estado
show_status() {
    echo "ðŸ“Š Estado Actual del Proyecto:"
    echo ""
    
    # Contar tests
    frontend_tests=$(find frontend/src -name "*.test.*" -o -name "*.spec.*" | wc -l)
    backend_tests=$(find backend/apps -name "test_*.py" -o -name "*_test.py" | wc -l)
    
    echo "ðŸ“ Tests encontrados:"
    echo "   Frontend: $frontend_tests tests"
    echo "   Backend: $backend_tests tests"
    echo ""
    
    # Estado de scripts
    echo "âš™ï¸ Scripts configurados:"
    if grep -q "test:critical" frontend/package.json; then
        echo "   âœ… test:critical configurado"
    else
        echo "   âŒ test:critical NO configurado"
    fi
    
    if grep -q "test:non-critical" frontend/package.json; then
        echo "   âœ… test:non-critical configurado"
    else
        echo "   âŒ test:non-critical NO configurado"
    fi
    
    # Estado de workflow
    echo ""
    echo "ðŸ”„ Workflow CI/CD:"
    if [ -f ".github/workflows/smart-testing.yml" ]; then
        echo "   âœ… Smart testing workflow configurado"
    else
        echo "   âŒ Smart testing workflow NO configurado"
    fi
}

# FunciÃ³n para nuevo mÃ³dulo
new_module_template() {
    echo "ðŸ“ Creando template para nuevo mÃ³dulo..."
    echo ""
    read -p "ðŸŽ¯ Nombre del mÃ³dulo (ej: auditorias, procesos): " module_name
    
    if [ -z "$module_name" ]; then
        echo "âŒ Nombre del mÃ³dulo requerido"
        exit 1
    fi
    
    # Crear directorio de documentaciÃ³n si no existe
    mkdir -p docs/testing/
    
    # Crear archivo de clasificaciÃ³n del mÃ³dulo
    cat > "docs/testing/${module_name}-test-classification.md" << EOF
# ðŸŽ¯ ClasificaciÃ³n de Tests - MÃ³dulo: ${module_name}

## âœ… TESTS CRÃTICOS

### Funcionalidad Core:
- [ ] \`src/hooks/__tests__/use${module_name^}.test.tsx\` - Hook principal del mÃ³dulo
- [ ] \`src/services/__tests__/${module_name}.service.test.ts\` - Service core
- [ ] \`src/__tests__/e2e/${module_name}-flow.test.tsx\` - Flujo end-to-end

### Backend:
- [ ] \`backend/apps/${module_name}/test_models.py\` - Modelos Django
- [ ] \`backend/apps/${module_name}/test_apis.py\` - APIs DRF

## âš ï¸ TESTS NO CRÃTICOS

### Limitaciones JSdom:
- [ ] Tests de file upload especÃ­ficos
- [ ] Tests de routing complejo

### UI/UX:
- [ ] Tests de componentes visuales
- [ ] Tests de animaciones

## ðŸ“ DECISIONES TOMADAS

### CrÃ­ticos confirmados:
- [Pendiente]

### No crÃ­ticos confirmados:
- [Pendiente]

## ðŸ“Š SCRIPTS

\`\`\`json
{
  "test:critical:${module_name}": "vitest run [rutas de tests crÃ­ticos]",
  "test:non-critical:${module_name}": "vitest run [rutas no crÃ­ticas] || true"
}
\`\`\`

## ðŸ“… FECHA DE CLASIFICACIÃ“N
$(date +"%Y-%m-%d")
EOF
    
    echo "âœ… Template creado en: docs/testing/${module_name}-test-classification.md"
    echo ""
    echo "ðŸ“‹ PrÃ³ximos pasos:"
    echo "1. Desarrollar tests del mÃ³dulo ${module_name}"
    echo "2. Ejecutar: ./scripts/quick-test-guide.sh analyze"
    echo "3. Clasificar tests usando el template creado"
    echo "4. Actualizar package.json con scripts especÃ­ficos"
}

# FunciÃ³n para clasificar test especÃ­fico
classify_test() {
    echo "ðŸ” Clasificador Interactivo de Tests"
    echo ""
    read -p "ðŸ“ Ruta del test a clasificar: " test_path
    
    if [ -z "$test_path" ]; then
        echo "âŒ Ruta del test requerida"
        exit 1
    fi
    
    if [ ! -f "$test_path" ]; then
        echo "âŒ Archivo no encontrado: $test_path"
        exit 1
    fi
    
    echo ""
    echo "ðŸŽ¯ Analizando: $test_path"
    echo ""
    
    # Preguntas de clasificaciÃ³n
    echo "ðŸ“‹ Responde las siguientes preguntas:"
    echo ""
    
    read -p "â“ Â¿Si este test falla, los usuarios no pueden usar funcionalidad core? (s/n): " q1
    read -p "â“ Â¿Es parte del flujo principal (happy path) del mÃ³dulo? (s/n): " q2
    read -p "â“ Â¿Afecta seguridad, autenticaciÃ³n o integridad de datos? (s/n): " q3
    read -p "â“ Â¿Funciona correctamente en browser real cuando falla en tests? (s/n): " q4
    read -p "â“ Â¿Es limitaciÃ³n de JSdom (routing, file API, DOM API)? (s/n): " q5
    
    echo ""
    echo "ðŸ“Š RESULTADO:"
    
    critical_score=0
    non_critical_score=0
    
    # Calcular score
    [[ "$q1" == "s" ]] && ((critical_score++))
    [[ "$q2" == "s" ]] && ((critical_score++))
    [[ "$q3" == "s" ]] && ((critical_score++))
    [[ "$q4" == "s" ]] && ((non_critical_score++))
    [[ "$q5" == "s" ]] && ((non_critical_score++))
    
    if [ $critical_score -ge 2 ]; then
        echo "âœ… RECOMENDACIÃ“N: TEST CRÃTICO"
        echo "   Debe incluirse en test:critical"
        echo "   Debe pasar al 100% para deployment"
    elif [ $non_critical_score -ge 1 ]; then
        echo "âš ï¸ RECOMENDACIÃ“N: TEST NO CRÃTICO"  
        echo "   Incluir en test:non-critical"
        echo "   Puede fallar sin bloquear deployment"
    else
        echo "ðŸ¤” RECOMENDACIÃ“N: REVISAR MANUALMENTE"
        echo "   Usar framework detallado en .github/TEST_CLASSIFICATION_FRAMEWORK.md"
    fi
    
    echo ""
    echo "ðŸ’¾ Documenta esta decisiÃ³n en docs/testing/[module]-test-classification.md"
}

# FunciÃ³n para ejecutar tests crÃ­ticos
run_critical() {
    echo "ðŸŽ¯ Ejecutando SOLO tests crÃ­ticos..."
    echo ""
    
    echo "ðŸ”§ Backend:"
    cd backend && python manage.py test --verbosity=2
    backend_result=$?
    
    echo ""
    echo "ðŸŽ¨ Frontend:"
    cd ../frontend && npm run test:critical
    frontend_result=$?
    
    echo ""
    if [ $backend_result -eq 0 ] && [ $frontend_result -eq 0 ]; then
        echo "âœ… TODOS LOS TESTS CRÃTICOS PASAN - LISTO PARA DEPLOYMENT! ðŸš€"
    else
        echo "âŒ TESTS CRÃTICOS FALLANDO - DEPLOYMENT BLOQUEADO ðŸš«"
        echo "ðŸ”§ Debe corregir tests crÃ­ticos antes de hacer merge"
    fi
}

# FunciÃ³n para smart testing
run_smart() {
    echo "ðŸ§  Ejecutando testing inteligente..."
    echo ""
    
    cd frontend && npm run test:all-with-smart-exit
    result=$?
    
    echo ""
    if [ $result -eq 0 ]; then
        echo "âœ… SMART TESTING COMPLETADO - LISTO PARA DEPLOYMENT! ðŸš€"
    else
        echo "âŒ TESTS CRÃTICOS FALLANDO - REVISAR ERRORES ðŸ”§"
    fi
}

# FunciÃ³n para generar reporte
generate_report() {
    echo "ðŸ“Š Generando reporte completo..."
    echo ""
    
    # Ejecutar anÃ¡lisis
    node scripts/classify-tests.js > /tmp/analysis.log 2>&1
    
    # Ejecutar tests crÃ­ticos
    echo "ðŸŽ¯ Ejecutando tests crÃ­ticos..."
    cd frontend && npm run test:critical > /tmp/critical.log 2>&1
    critical_result=$?
    
    # Crear reporte
    cat > /tmp/zentraqms-test-report.md << EOF
# ðŸ“Š Reporte de Testing - ZentraQMS
**Fecha**: $(date +"%Y-%m-%d %H:%M:%S")

## ðŸŽ¯ Tests CrÃ­ticos
**Estado**: $([ $critical_result -eq 0 ] && echo "âœ… PASANDO" || echo "âŒ FALLANDO")

### Detalles:
\`\`\`
$(tail -20 /tmp/critical.log)
\`\`\`

## ðŸ” AnÃ¡lisis de ClasificaciÃ³n
\`\`\`
$(cat /tmp/analysis.log)
\`\`\`

## ðŸ“‹ Recomendaciones
$([ $critical_result -eq 0 ] && echo "âœ… Listo para deployment" || echo "ðŸ”§ Corregir tests crÃ­ticos antes de merge")

EOF
    
    echo "ðŸ“ Reporte generado en: /tmp/zentraqms-test-report.md"
    echo ""
    echo "ðŸ“‹ Vista previa:"
    head -20 /tmp/zentraqms-test-report.md
}

# Procesar argumentos
case "$1" in
    "analyze")
        analyze_tests
        ;;
    "check")
        check_critical
        ;;
    "status")
        show_status
        ;;
    "new-module")
        new_module_template
        ;;
    "classify")
        classify_test
        ;;
    "critical")
        run_critical
        ;;
    "smart")
        run_smart
        ;;
    "report")
        generate_report
        ;;
    "help"|"-h"|"--help"|"")
        show_help
        ;;
    *)
        echo "âŒ Comando no reconocido: $1"
        echo ""
        show_help
        exit 1
        ;;
esac